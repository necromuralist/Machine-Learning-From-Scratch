#+BEGIN_COMMENT
.. title: Deep Learning
.. slug: deep-learning
.. date: 2020-04-28 15:38:59 UTC-07:00
.. tags: deep learning
.. category: Deep Learning
.. link: 
.. description: A simple sketch of some deep learning ideas.
.. type: text
.. status: 
.. updated: 

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 5
#+PROPERTY: header-args :session /home/athena/.local/share/jupyter/runtime/kernel-98d5ffa8-ef80-4a63-a818-cc4956f1b0be.json
#+BEGIN_SRC python :results none :exports none
%load_ext autoreload
%autoreload 2
#+END_SRC
* Beginning
** Imports
*** Python
#+begin_src python :results none
import os
import pickle
import sys
import time
#+end_src
*** From PyPi
#+begin_src python :results none
from dotenv import load_dotenv
import numpy
#+end_src
*** Coursera
#+begin_src python :results none
from ml_from_scratch.coursera.precode import init_subset
#+end_src
** Set Up
*** The Random Seed
#+begin_src python :results none
numpy.random.seed(1000)
#+end_src
*** Student ID
#+begin_src python :results none
load_dotenv()
STUDENT_ID = os.environ["STUDENT_ID"]
#+end_src
* Middle
** Define The Network
*** Define the Convolutional Layer
 #+begin_src python :results none
class Convolution2D:
    """A 2D Convolutional layer

    Args:
     inputs_channel: number of channels (e.g. 3 for RGB)
     num_filters: how many kernels to create
     kernel_size: how large a kernel to use
     padding: rows and columns to pad the edges for the convolution
     stride: how much to move the kernel with each step
     learning_rate: constant to apply to the back-propagation
     name: and identifier for the layer
    """
    # Initialization of convolutional layer
    def __init__(self, inputs_channel, num_filters, kernel_size, padding, stride, learning_rate, name):
        # weight size: (F, C, K, K)
        # bias size: (F) 
        self.F = num_filters
        self.K = kernel_size
        self.C = inputs_channel

        self.weights = numpy.zeros((self.F, self.C, self.K, self.K))
        self.bias = numpy.zeros((self.F, 1))
        for i in range(0,self.F):
            self.weights[i,:,:,:] = numpy.random.normal(loc=0, scale=numpy.sqrt(1./(self.C*self.K*self.K)), size=(self.C, self.K, self.K))

        self.p = padding
        self.s = stride
        self.lr = learning_rate
        self.name = name
    
    # Padding Layer 
    def zero_padding(self, inputs, size):
        w, h = inputs.shape[0], inputs.shape[1]
        new_w = 2 * size + w
        new_h = 2 * size + h
        out = numpy.zeros((new_w, new_h))
        out[size:w+size, size:h+size] = inputs
        return out
    
    # Forward propagation
    def forward(self, inputs):
        # input size: (C, W, H)
        # output size: (N, F ,WW, HH)
        C = inputs.shape[0]
        W = inputs.shape[1]+2*self.p
        H = inputs.shape[2]+2*self.p
        self.inputs = numpy.zeros((C, W, H))
        for c in range(inputs.shape[0]):
            self.inputs[c,:,:] = self.zero_padding(inputs[c,:,:], self.p)
        WW = (W - self.K)//self.s + 1
        HH = (H - self.K)//self.s + 1
        feature_maps = numpy.zeros((self.F, WW, HH))
        for f in range(self.F):
            for w in range(WW):
                for h in range(HH):
                    feature_maps[f,w,h]=numpy.sum(self.inputs[:,w:w+self.K,h:h+self.K]*self.weights[f,:,:,:])+self.bias[f]

        return feature_maps
    
    # Backward Propagation
    def backward(self, dy):

        C, W, H = self.inputs.shape
        dx = numpy.zeros(self.inputs.shape)
        dw = numpy.zeros(self.weights.shape)
        db = numpy.zeros(self.bias.shape)

        F, W, H = dy.shape
        for f in range(F):
            for w in range(W):
                for h in range(H):
                    dw[f,:,:,:]+=dy[f,w,h]*self.inputs[:,w:w+self.K,h:h+self.K]
                    dx[:,w:w+self.K,h:h+self.K]+=dy[f,w,h]*self.weights[f,:,:,:]

        for f in range(F):
            db[f] = numpy.sum(dy[f, :, :])

        self.weights -= self.lr * dw
        self.bias -= self.lr * db
        return dx
    
    # Function for extract the weights and bias for storage
    def extract(self):
        return {self.name+'.weights':self.weights, self.name+'.bias':self.bias}
    
    # Feed the pretrained weights and bias for models 
    def feed(self, weights, bias):
        self.weights = weights
        self.bias = bias
 #+end_src
*** Max Pooling Layer
 #+begin_src python :results none
class Maxpooling2D:
    # Initialization of MaxPooling layer
    def __init__(self, pool_size, stride, name):
        self.pool = pool_size
        self.s = stride
        self.name = name
    
    # Forward propagation
    def forward(self, inputs):
        self.inputs = inputs
        C, W, H = inputs.shape
        new_width = (W - self.pool)//self.s + 1
        new_height = (H - self.pool)//self.s + 1
        out = numpy.zeros((C, new_width, new_height))
        for c in range(C):
            for w in range(W//self.s):
                for h in range(H//self.s):
                    out[c, w, h] = numpy.max(self.inputs[c, w*self.s:w*self.s+self.pool, h*self.s:h*self.s+self.pool])
        return out
    
    # Backward propagation
    def backward(self, dy):
        C, W, H = self.inputs.shape
        dx = numpy.zeros(self.inputs.shape)
        
        for c in range(C):
            for w in range(0, W, self.pool):
                for h in range(0, H, self.pool):
                    st = numpy.argmax(self.inputs[c,w:w+self.pool,h:h+self.pool])
                    (idx, idy) = numpy.unravel_index(st, (self.pool, self.pool))
                    dx[c, w+idx, h+idy] = dy[c, w//self.pool, h//self.pool]
        return dx
    
    # No weights and bias for pooling layer to store
    def extract(self):
        return 
 #+end_src
*** Fully Connected Layer
 #+begin_src python :results none
class FullyConnected:
    # Initialization of Fully-Connected Layer
    def __init__(self, num_inputs, num_outputs, learning_rate, name):
        self.weights = 0.01*numpy.random.rand(num_inputs, num_outputs)
        self.bias = numpy.zeros((num_outputs, 1))
        self.lr = learning_rate
        self.name = name
    
    # Forward Propagation
    def forward(self, inputs):
        self.inputs = inputs
        return numpy.dot(self.inputs, self.weights) + self.bias.T
    
    # Backward Propagation
    def backward(self, dy):

        if dy.shape[0] == self.inputs.shape[0]:
            dy = dy.T
        dw = dy.dot(self.inputs)
        db = numpy.sum(dy, axis=1, keepdims=True)
        dx = numpy.dot(dy.T, self.weights.T)

        self.weights -= self.lr * dw.T
        self.bias -= self.lr * db

        return dx
    
    # Extract weights and bias for storage
    def extract(self):
        return {self.name+'.weights':self.weights, self.name+'.bias':self.bias}
    
    # Feed the pretrained weights and bias for models 
    def feed(self, weights, bias):
        self.weights = weights
        self.bias = bias
 #+end_src
*** Flatten
 #+begin_src python :results none
class Flatten:
    def __init__(self):
        pass
    def forward(self, inputs):
        self.C, self.W, self.H = inputs.shape
        return inputs.reshape(1, self.C*self.W*self.H)
    def backward(self, dy):
        return dy.reshape(self.C, self.W, self.H)
    def extract(self):
        return
 #+end_src
*** Activation Function (ReLU)
 #+begin_src python :results none
class ReLu:
    def __init__(self):
        pass
    def forward(self, inputs):
        self.inputs = inputs
        ret = inputs.copy()
        ret[ret < 0] = 0
        return ret
    def backward(self, dy):
        dx = dy.copy()
        dx[self.inputs < 0] = 0
        return dx
    def extract(self):
        return
 #+end_src
*** Softmax
 #+begin_src python :results none
class Softmax:
    def __init__(self):
        pass
    def forward(self, inputs):
        exp = numpy.exp(inputs, dtype=numpy.float)
        self.out = exp/numpy.sum(exp)
        return self.out
    def backward(self, dy):
        return self.out.T - dy.reshape(dy.shape[0],1)
    def extract(self):
        return
 #+end_src
*** Cross-Entropy Loss
 #+begin_src python :results none
def cross_entropy(inputs, labels):
    out_num = labels.shape[0]
    p = numpy.sum(labels.reshape(1,out_num)*inputs)
    loss = -numpy.log(p)
    return loss
 #+end_src
*** Finally, the Neural Network
 This step shows how to define a simple CNN with all kind of layers which we introduced above.
 #+begin_src python :results none
class Net:
    def __init__(self):
        # input: 28x28
        # output: 1x4 (only a subset, containing 4 classes, of the MNIST will be used)
        # conv1:  {(28-5+0x0)/2+1} -> (12x12x6) (output size of convolutional layer)
        # maxpool2: {(12-2)/2+1} -> (6x6)x6 (output size of pooling layer)
        # fc3: 216 -> 32
        # fc4: 32 -> 4
        # softmax: 4 -> 4
        lr = 0.001
        self.layers = []
        self.layers.append(Convolution2D(inputs_channel=1, num_filters=6, kernel_size=5, padding=0, stride=2, learning_rate=lr, name='conv1'))
        self.layers.append(ReLu())
        self.layers.append(Maxpooling2D(pool_size=2, stride=2, name='maxpool2'))
        self.layers.append(Flatten())
        self.layers.append(FullyConnected(num_inputs=6*6*6, num_outputs=32, learning_rate=lr, name='fc3'))
        self.layers.append(ReLu())
        self.layers.append(FullyConnected(num_inputs=32, num_outputs=4, learning_rate=lr, name='fc4'))
        self.layers.append(Softmax())
        self.lay_num = len(self.layers)
    
    ### Function for train the network
    def train(self, data, label):
        batch_size = data.shape[0]
        loss = 0
        acc = 0
        for b in range(batch_size):
            x = data[b]
            y = label[b]
            # forward pass
            for l in range(self.lay_num):
                output = self.layers[l].forward(x)
                x = output
            loss += cross_entropy(output, y)
            if numpy.argmax(output) == numpy.argmax(y):
                acc += 1
            # backward pass
            dy = y
            for l in range(self.lay_num-1, -1, -1):
                dout = self.layers[l].backward(dy)
                dy = dout
        return loss, acc
 #+end_src
** Load the Data
The subset of MNIST is created based on the last 4 digits of your student. There are 4 categories and all returned 
samples are preprocessed and shuffled. 

#+begin_src python :results none
sub_train_images, sub_train_labels, sub_test_images, sub_test_labels = init_subset(STUDENT_ID)
#+end_src
** Initialize the Network
#+begin_src python :results none
net = Net()
epoch = 10            ### Default number of epochs
batch_size = 100      ### Default batch size
num_batch = sub_train_images.shape[0]/batch_size

test_size = sub_test_images.shape[0]      # Obtain the size of testing samples
train_size = sub_train_images.shape[0]    # Obtain the size of training samples
#+end_src

Please compile your own evaluation code based on the training code 
to evaluate the trained network.
The function name and the inputs of the function have been predifined and please finish the remaining part.
#+begin_src python :results none
def evaluate(net, images, labels):
    correct = 0    
    loss = 0
    batch_size = 1

    for batch_index in range(0, images.shape[0], batch_size):
        x = images[batch_index]
        y = labels[batch_index]
        for layer in range(net.lay_num):
            x = net.layers[layer].forward(x)
        loss += cross_entropy(x, y)
        if numpy.argmax(x) == numpy.argmax(y):
            correct += 1
    return correct/len(images), loss/len(images)
#+end_src
** Train
#+begin_src python :results output :exports both
for e in range(epoch):
    total_acc = 0    
    total_loss = 0
    print('Epoch %d' % e)
    for batch_index in range(0, sub_train_images.shape[0], batch_size):
        # batch input
        if batch_index + batch_size < sub_train_images.shape[0]:
            data = sub_train_images[batch_index:batch_index+batch_size]
            label = sub_train_labels[batch_index:batch_index + batch_size]
        else:
            data = sub_train_images[batch_index:sub_train_images.shape[0]]
            label = sub_train_labels[batch_index:sub_train_labels.shape[0]]
        # Compute the remaining time
        start_time = time.time()
        batch_loss,batch_acc = net.train(data, label)  # Train the network with samples in one batch 
        
        end_time = time.time()
        batch_time = end_time-start_time
        remain_time = (sub_train_images.shape[0]-batch_index)/batch_size*batch_time
        hrs = int(remain_time/3600)
        mins = int((remain_time/60-hrs*60))
        secs = int(remain_time-mins*60-hrs*3600)
        # print('=== Iter:{0:d} === Remain: {1:d} Hrs {2:d} Mins {3:d} Secs ==='.format(int(batch_index+batch_size),int(hrs),int(mins),int(secs)))
    train_acc, train_loss = evaluate(net, sub_train_images, sub_train_labels)  # Use the evaluation code to obtain the training accuracy and loss
    test_acc, test_loss = evaluate(net, sub_test_images, sub_test_labels)      # Use the evaluation code to obtain the testing accuracy and loss
    print('=== Epoch:{0:d} Train Size:{1:d}, Train Acc:{2:.3f}, Train Loss:{3:.3f} ==='.format(e, train_size,train_acc,train_loss))

    print('=== Epoch:{0:d} Test Size:{1:d}, Test Acc:{2:.3f}, Test Loss:{3:.3f} ==='.format(e, test_size, test_acc,test_loss))
#+end_src

#+RESULTS:
#+begin_example
Epoch 0
=== Epoch:0 Train Size:2000, Train Acc:0.970, Train Loss:0.103 ===
=== Epoch:0 Test Size:400, Test Acc:0.912, Test Loss:0.301 ===
Epoch 1
=== Epoch:1 Train Size:2000, Train Acc:0.968, Train Loss:0.104 ===
=== Epoch:1 Test Size:400, Test Acc:0.912, Test Loss:0.313 ===
Epoch 2
=== Epoch:2 Train Size:2000, Train Acc:0.962, Train Loss:0.109 ===
=== Epoch:2 Test Size:400, Test Acc:0.890, Test Loss:0.330 ===
Epoch 3
=== Epoch:3 Train Size:2000, Train Acc:0.960, Train Loss:0.107 ===
=== Epoch:3 Test Size:400, Test Acc:0.895, Test Loss:0.330 ===
Epoch 4
=== Epoch:4 Train Size:2000, Train Acc:0.961, Train Loss:0.107 ===
=== Epoch:4 Test Size:400, Test Acc:0.895, Test Loss:0.343 ===
Epoch 5
=== Epoch:5 Train Size:2000, Train Acc:0.962, Train Loss:0.107 ===
=== Epoch:5 Test Size:400, Test Acc:0.897, Test Loss:0.346 ===
Epoch 6
=== Epoch:6 Train Size:2000, Train Acc:0.961, Train Loss:0.107 ===
=== Epoch:6 Test Size:400, Test Acc:0.890, Test Loss:0.363 ===
Epoch 7
=== Epoch:7 Train Size:2000, Train Acc:0.962, Train Loss:0.103 ===
=== Epoch:7 Test Size:400, Test Acc:0.892, Test Loss:0.367 ===
Epoch 8
=== Epoch:8 Train Size:2000, Train Acc:0.955, Train Loss:0.113 ===
=== Epoch:8 Test Size:400, Test Acc:0.892, Test Loss:0.397 ===
Epoch 9
=== Epoch:9 Train Size:2000, Train Acc:0.961, Train Loss:0.103 ===
=== Epoch:9 Test Size:400, Test Acc:0.892, Test Loss:0.390 ===
#+end_example

* End
