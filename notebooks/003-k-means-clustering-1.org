#+BEGIN_COMMENT
.. title: K-Means Clustering 1
.. slug: k-means-clustering-1
.. date: 2020-04-12 23:19:50 UTC-07:00
.. tags: project,clustering,unsupervised
.. category: Project
.. link: 
.. description: K-Means Clustering using random initial points.
.. type: text
.. status: 
.. updated: 

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 5
#+PROPERTY: header-args :session /home/athena/.local/share/jupyter/runtime/kernel-9dcdfa11-8961-45a4-be1c-73970e278b88.json
#+BEGIN_SRC python :results none :exports none
%load_ext autoreload
%autoreload 2
#+END_SRC
* Beginning
  This is part 1 of the K-means clustering project. In this portion we'll implement K-means classification using random starting centroids.
** Imports
*** Python
#+begin_src python :results none
from collections import defaultdict, namedtuple
from functools import partial
from pathlib import Path
import os
#+end_src
*** PyPi
#+begin_src python :results none
from expects import (
    be_true,
    expect
)
import dotenv
import hvplot.pandas
import matplotlib.pyplot as pyplot
import numpy
import pandas
import seaborn
#+end_src
*** Coursera Code
#+begin_src python :results none
from ml_from_scratch.coursera.Precode import initial_S1
#+end_src
*** Others
#+begin_src python :results none
from graeae import EmbedHoloviews
#+end_src
** Plotting
#+begin_src python :results none
SLUG = "k-means-clustering-1"
OUTPUT_PATH = Path(f"../files/notebooks/{SLUG}")
#+end_src

#+begin_src python :results none
Embed = partial(EmbedHoloviews, folder_path=OUTPUT_PATH)
#+end_src
#+BEGIN_SRC python :results none
get_ipython().run_line_magic('matplotlib', 'inline')
get_ipython().run_line_magic('config', "InlineBackend.figure_format = 'retina'")
seaborn.set(style="whitegrid",
            rc={"axes.grid": False,
                "xtick.labelsize": 10,
                "ytick.labelsize": 10,
                "font.size": 12,
                "font.family": ["sans-serif"],
                "font.sans-serif": ["Open Sans", "Latin Modern Sans", "Lato"],
                "figure.figsize": (10, 8)},
            font_scale=3)
#+END_SRC
** The Data
#+begin_src python :results output :exports both
dotenv.load_dotenv(override=True)
DATA_PATH = Path(os.environ["KNN_DATA"]).expanduser()
data = numpy.load(DATA_PATH)
frame = pandas.DataFrame(data, columns=["x", "y"])
print(data.shape)
#+end_src

#+RESULTS:
: (300, 2)

* Middle
** Some Plotting
*** Looking at the Distribution
#+begin_src python :results none :file ../files/notebooks/k-means-clustering-1/data_distribution.png
figure, axe = pyplot.subplots()
#plot = frame.plot.kde()
figure.suptitle("Data Distribution", weight="bold")
seaborn.distplot(frame, rug=True, ax=axe)
#+end_src

[[file:data_distribution.png]]

It looks like there might be at least three sub-populations there.

#+begin_src python :results none :file ../files/notebooks/k-means-clustering-1/scatter.png
figure, axe = pyplot.subplots()

figure.suptitle("X vs Y", weight="bold")
seaborn.relplot(x="x", y="y", data=frame, ax=axe)
#+end_src

[[file:scatter.png]]

Well, this makes it look like there's four to six groups.
** Coursera's Splitting
   The =initial_S1= function appears to be an example function that returns /k/ and the initial centroids that were randomly chosen for the data. It uses ~k=3~ and ~k=5~.
#+begin_src python :results output :exports both
STUDENT_ID = os.environ["STUDENT_ID"]
INDEX_ID = int(STUDENT_ID) % 150 
# please replace 0111 with your last four digit of your ID
k1, i_point1, k2, i_point2 = initial_S1(STUDENT_ID, data) 
print(f"K={k1}")
print(i_point1)
print(f"\nK={k2}")
print(i_point2)
#+end_src

#+RESULTS:
#+begin_example
Strategy 1: k and initial points
K=3
[[3.79752017 0.69134312]
 [3.81485895 6.91844078]
 [2.25790845 7.44778003]]

K=5
[[9.26998864 9.62492869]
 [6.05509889 7.23007608]
 [7.74867074 1.71812324]
 [7.25412082 2.77862318]
 [7.57805025 3.82487017]]
#+end_example

So for the initialization it appears we don't have to implement anything other than something that calls =initial_point_idx(id, k, N)= and =init_point(data, idx)=. 

**Note:** On re-reading the notes it appears these values are for the submission only. To implement k-means for the plotting we need to use random values (the function to get the index sets the seed so it returns the same values every time.)
*** More on the functions
    Since there's no real documentation for it I thought I should note what the signatures are for the functions.
**** initial_point_idx
     This function creates an array of indices for the centroids using [[https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.random.permutation.html][numpy.random.permutation]]. In this case we're passing it the number of rows in the data so it's permuting an [[https://docs.scipy.org/doc/numpy/reference/generated/numpy.arange.html][arange]] - integers from 0 to number of rows - 1.

| Argument | Description                       |
|----------+-----------------------------------|
| id       | The student ID passed in          |
| k        | The number of centroids to create |
| N        | The number of rows in the data    |

It returns the array of random indices it created.
*** init_point 
    This converts the indices created by =initial_point_idx= and retrieves the actual points from the data.

| Argument | Description                      |
|----------+----------------------------------|
| data     | Our array of points.             |
| idx      | An array of indices for the data |

It returns the array of coordinates for the initial centroids.
** Getting Updated Centroids
*** Clusters
    This is just a namedtuple to hold the clusters and centroids.
#+begin_src python :results none
Clusters = namedtuple("Clusters", ["clusters", "centroids"])
#+end_src
*** Get Centroids
#+begin_src python :results none
def update_centroids(centroids: numpy.array, points: numpy.array) -> Clusters:
    """Get updated centroids
    
    Args:
     centroids: previous set of centroids
     points: points to cluster (shape = n rows, 2 columns)

    Returns:
     Clusters namedtuple
    """
    clusters = defaultdict(list)
    for point in points:
        distances = numpy.sum(numpy.square(point - centroids), axis=1)
        closest = numpy.argmin(distances)
        clusters[closest].append(point)
    clusters = numpy.array(list(clusters.values()))
    assert len(clusters) == len(centroids), f"clusters: {clusters}"
    centroids= numpy.array([
        numpy.mean(cluster, axis=0)
        for cluster in clusters
    ])
    return Clusters(centroids=centroids, clusters=clusters)
#+end_src
*** Test the Get Centroids
Feature: A function to get updated centroids

Scenario: Updated centroids are retrieved
Given an array of points

#+begin_src python :results none
test = numpy.array([
    [0, 0],
    [1, 1],
    [3, 3],
    [4, 4]
])
#+end_src

And some initial centroids
#+begin_src python :results none
centroids = numpy.array([
    test[-1],
    test[0]
])
#+end_src

When the update centroids are retrieved
#+begin_src python :results output :exports both
updated = update_centroids(points=test, centroids=centroids)
print(updated.centroids)
expected = numpy.array([
    [0.5, 0.5],
    [3.5, 3.5],
])
expect(numpy.allclose(updated.centroids, expected)).to(be_true)
print((numpy.sort(centroids) == numpy.sort(updated.centroids)).all())
#+end_src

#+RESULTS:
: [[0.5 0.5]
:  [3.5 3.5]]
: False

#+begin_src python :results output :exports both
update_2 = update_centroids(points=test,
                            centroids=updated.centroids)
print(update_2.centroids)
#+end_src

#+RESULTS:
: [[0.5 0.5]
:  [3.5 3.5]]
** The Objective Function
   The objective of training is to minimize the variation within a cluster. In this case we're measuring that as the sum of the squared distances from the points within a cluster to the mean of the cluster.

\[
\sum_{i=1}^K \sum_{x \in D_i} \lVert x - \mu_i \rVert^2
\]

#+begin_src python :results none
def objective(clusters: numpy.array) -> float:
    """calculates the variation within clusters

    Args:
     clusters: array (or other iterable of points in the clusters)

    Returns:
     the score for the cluster variation
    """
    variation = 0
    for cluster in clusters:
        center = numpy.mean(cluster)
        variation += (
            numpy.sum(
                numpy.square(
                    cluster - center
                )
            )
        )
    return variation
#+end_src

#+begin_src python :results output :exports both
variation = objective(updated.clusters)
print(variation)
#+end_src

#+RESULTS:
: 2.0

** Getting the Random Indices

#+begin_src python :results none
def random_points(k: int, data: numpy.array) -> numpy.array:
    """Gets random points from the data

    Args:
     k: number of points to get
     data: the source of the points

    Returns:
     k randomly selected points from the data
    """
    return data[np.random.permutation(len(data))[:k], :]
#+end_src
** Putting Them All Together

#+begin_src python :results none
def k_means(k: int, data: numpy.array,
            centroids: numpy.array=None) -> Clusters:
    """Calculates the k-means clusters and the variance


    Args:
     k: number of clusters to create
     data: the source of the points to cluster
     initial_centroids: pre-chosen initial centroids (otherwise random)
    """
    centroids = (centroids if centroids is not None
                 else random_points(data=data, k=k))
    assert len(centroids) == k
    while True:
        updated = update_centroids(centroids, data)
        if (numpy.sort(centroids) == numpy.sort(updated.centroids)).all():
            break
        centroids = updated.centroids
    return updated
#+end_src

** Running K-means
#+begin_src python :results output :exports both
outcome = k_means(2, test)
print(outcome)
print(objective(outcome.clusters))
#+end_src

#+RESULTS:
: Clusters(clusters=array([[[0, 0],
:         [1, 1]],
: 
:        [[3, 3],
:         [4, 4]]]), centroids=array([[0.5, 0.5],
:        [3.5, 3.5]]))
: 2.0
** K From Two To Ten
#+begin_src python :results none
Outcomes = namedtuple("Outcomes", ["losses", "points"])
#+end_src

#+begin_src python :results none
def grid_search(minimum_k: int=2,
                maximum_k: int=10,
                data: numpy.array=data) -> Outcomes:
    """Runs K-means over a range of K"""
    losses = {}
    outcomes = {}
    for k in range(minimum_k, maximum_k + 1):
        outcome = k_means(k, data)
        loss = objective(outcome.clusters)
        losses[k] = loss
        outcomes[k] = outcome
    losses = pandas.DataFrame(dict(K=list(losses.keys()),
                                   Loss=list(losses.values())))
    return Outcomes(losses=losses, points=outcomes)
#+end_src

#+begin_src python :results none
outcomes = grid_search()

plot = outcomes.losses.hvplot.bar(x="K", y="Loss").opts(
    height=800, width=1000, title="Loss by K")
embedded = Embed(plot=plot, file_name="loss_vs_k")()
#+end_src

#+begin_src python :results output html :exports both
print(embedded)
#+end_src

#+RESULTS:
#+begin_export html
: <object type="text/html" data="loss_vs_k.html" style="width:100%" height=800>
:   <p>Figure Missing</p>
: </object>
#+end_export

#+begin_src python :results none
data_frame = pandas.DataFrame(data, columns=["x", "y"])
ten_frame = pandas.DataFrame(outcomes.points[10].centroids, columns=["x", "y"])
#+end_src

#+begin_src python :results none
def plot_clusters(outcomes: Outcomes, k: int, data: numpy.array=data):
    data_frame = pandas.DataFrame(data, columns=["x", "y"])
    center_frame = pandas.DataFrame(outcomes.points[k].centroids,
                                    columns=["x", "y"])

    plots = center_frame.hvplot.scatter(
        x="x", y="y",
        s=100, c="k", marker="+")

    for cluster in outcomes.points[k].clusters:
        c_frame = pandas.DataFrame(cluster, columns=["x", "y"])
        plots *= c_frame.hvplot.scatter(x="x", y="y")
        
    plot = (plots).opts(width=1000, height=800, title=f"Clusters K={k}")
    return Embed(plot=plot, file_name=f"clusters_{k}")()
#+end_src

#+begin_src python :results none
embedded = plot_clusters(outcomes, 10)
#+end_src

#+begin_src python :results output html :exports both
print(embedded)
#+end_src

#+RESULTS:
#+begin_export html
<object type="text/html" data="clusters_10.html" style="width:100%" height=800>
  <p>Figure Missing</p>
</object>
#+end_export

* End
** Submissions
*** Plotting the two runs
#+begin_src python :results none :file ../files/notebooks/k-means-clustering-1/strategy_1_run_one.png
outcomes_2 = grid_search()
figure, axe = pyplot.subplots()
figure.suptitle("Strategy One", weight="bold")
# axe.set_title("Strategy One")
outcomes.losses["Run"] = "First"
outcomes_2.losses["Run"] = "Second"

outcomes.losses.plot(x="K", y="Loss", label="First Run", ax=axe)
outcomes_2.losses.plot(x="K", y="Loss", label="Second Run", ax=axe)
axe.set_ylabel("Objective Function")
#+end_src
    
[[file:strategy_1_run_one.png]]

It kind of looks like either k=4 or k=5 is the actual optimum (based on the assumption that the biggest drop indicates the right level).

#+begin_src python :results none
embedded = plot_clusters(outcomes_2, 4)
#+end_src

#+begin_src python :results output html :exports both
print(embedded)
#+end_src

#+RESULTS:
#+begin_export html
<object type="text/html" data="clusters_4.html" style="width:100%" height=800>
  <p>Figure Missing</p>
</object>
#+end_export

#+begin_src python :results none
embedded = plot_clusters(outcomes_2, 5)
#+end_src

#+begin_src python :results output html :exports both
print(embedded)
#+end_src

#+RESULTS:
#+begin_export html
<object type="text/html" data="clusters_5.html" style="width:100%" height=800>
  <p>Figure Missing</p>
</object>
#+end_export

#+begin_src python :results none
embedded = plot_clusters(outcomes_2, 6)
#+end_src

#+begin_src python :results output html :exports both
print(embedded)
#+end_src

#+RESULTS:
#+begin_export html
<object type="text/html" data="clusters_6.html" style="width:100%" height=800>
  <p>Figure Missing</p>
</object>
#+end_export

#+begin_src python :results none
embedded = plot_clusters(outcomes_2, 7)
#+end_src

#+begin_src python :results output html :exports both
print(embedded)
#+end_src

#+RESULTS:
#+begin_export html
<object type="text/html" data="clusters_7.html" style="width:100%" height=800>
  <p>Figure Missing</p>
</object>
#+end_export


#+begin_src python :results none
embedded = plot_clusters(outcomes_2, 8)
#+end_src

#+begin_src python :results output html :exports both
print(embedded)
#+end_src

#+RESULTS:
#+begin_export html
: <object type="text/html" data="clusters_8.html" style="width:100%" height=800>
:   <p>Figure Missing</p>
: </object>
#+end_export


** The ID-Specific Data
   For the numeric submission we need to run k-means over the two points that their pre-defined functions produced.

#+begin_src python :results none
Submission = namedtuple("Submission", ["clusters", "centroids", "loss"])
#+end_src

#+begin_src python :results none
def check_centroids(centroids: numpy.array,
                    data: numpy.array=data) -> Submission:
    """Check the outcome with pre-initialized centroids

    Args:
     centroids: initial centroids for the k-means algorithm
     data: points to cluster
    """
    outcome = k_means(k=len(centroids), centroids=centroids, data=data)
    score = objective(outcome.clusters)
    return Submission(clusters=outcome.clusters, centroids=outcome.centroids,
                      loss=score)
#+end_src
*** First Check

#+begin_src python :results output :exports both
outcome_3 = check_centroids(centroids=i_point1)
print(f"K: {k1}")
print(f"Centroids:\n{outcome_3.centroids}")
print(f"\nObjective Function: {outcome_3.loss}")
#+end_src

#+RESULTS:
: K: 3
: Centroids:
: [[2.56146449 6.08861338]
:  [6.49724962 7.52297293]
:  [5.47740039 2.25498103]]
: 
: Objective Function: 2526.605733937463

*** Second Check
#+begin_src python :results output :exports both
outcome_4 = check_centroids(centroids=i_point2)
print(f"K: {k2}")
print(f"Centroids:\n{outcome_4.centroids}")
print(f"\nObjective Function: {outcome_4.loss}")
#+end_src

#+RESULTS:
: K: 5
: Centroids:
: [[3.22202355 7.15937996]
:  [7.49365367 8.52417952]
:  [7.55616782 2.23516796]
:  [5.37514379 4.53101654]
:  [2.68198633 2.09461587]]
: 
: Objective Function: 2069.2452603873444

